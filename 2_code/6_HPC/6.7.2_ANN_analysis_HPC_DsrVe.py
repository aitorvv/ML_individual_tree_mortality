# /usr/bin/env python3
# -*- coding: utf-8 -*-
"""ANN_model7_r33-HPC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11nh2vrw6JVuSy-Jey91j9ffLIXwi_0Mz

# Neural network to predict individual tree mortality

Simulation using the candidate model TEST_7

ðŸ”‘ **Note**: to use a different radii you have to change all the paths, the rest is fine

### Import libraries
"""

import pandas as pd
import numpy  as np
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.utils import class_weight

import json
from os import listdir
from os.path import isfile, join
import time

"""### Get input files"""

# Selection section 

df_size = 'small_random'  # small, small_random, medium, medium_random, big
var_size = 'easy'  # easy, medium, hard, extreme

# get all the input files on lists

path_train = '/home/aitorvazquez/PhD/Vitality/3_final/data/5_analysis/ann/input/D{}_V{}/train/'.format(df_size, var_size)
path_test = '/home/aitorvazquez/PhD/Vitality/3_final/data/5_analysis/ann/input/D{}_V{}/test/'.format(df_size, var_size)

train_files = [f for f in listdir(path_train) if isfile(join(path_train, f))]
test_files = [f for f in listdir(path_test) if isfile(join(path_test, f))]

#len(train_files), len(test_files)

"""# Create functions for each neural network model

### TEST_7: the model which provide better results taking into account dead trees
"""

# model fitting test function

METRICS = [
      keras.metrics.TruePositives(name='tp'),
      keras.metrics.FalsePositives(name='fp'),
      keras.metrics.TrueNegatives(name='tn'),
      keras.metrics.FalseNegatives(name='fn'),
      keras.metrics.BinaryAccuracy(name='accuracy'),
      keras.metrics.Precision(name='precision'),
      keras.metrics.Recall(name='recall'),
      keras.metrics.AUC(name='auc'),
      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve
]


def fit_test_model_7(X_train, y_train, X_test, y_test, X_val, y_val, weights):
  """
  Function to fit an ANN model for each feature combination and make predictions.
  The output is a list of values for their predictions.
  """


  # 0. Set random seed for reproducibility
  tf.random.set_seed(33)

  # 1. Create the model using the Sequential API
  ann_model = tf.keras.Sequential([
    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train.shape[-1],)),
    tf.keras.layers.Dropout(0.3), # dropout for skip overfitting
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dropout(0.3), # dropout for skip overfitting
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1, activation = 'sigmoid') # output layer
  ])

  # 2. Compile the model
  ann_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), # binary since we are working with 2 clases (0 & 1)
                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
                  metrics = METRICS) # precision-recall curve

  # 2.1. Configure early stopping
 # es = EarlyStopping(
 #     monitor='val_loss', # precision-recall curve
 #     #min_delta=0.0002, # minimium amount of change to count as an improvement
 #     verbose=1,
 #     patience=200, # how many epochs to wait before stopping
 #     mode='min', # stop when our metric stop to decrease
 #     restore_best_weights=True,
 #     start_from_epoch=1
 # )


  # 3. Fit the model
  history = ann_model.fit(X_train,
                          y_train,
                          epochs=3000,
                          batch_size=round(len(X_train)/10),
                          class_weight=weights,
                          validation_data=(X_val, y_val),
#                          callbacks=[es])  # callback to avoid overfitting
                          verbose=0)

  # 4. Get model predictions
  preds = ann_model.predict(X_test)

  return history, preds

"""# Get predictions for each feature combination"""

# LET'S GO! - Original

# dictionary for my results
my_results = dict()

# get time
time_list = dict()
start_ann = time.time()

for combi in range(len(train_files)):

  # feature combination index
  combi += 1

  # files index
  model_index = ['ann_model_', str(combi), '.csv']
  model_index = ''.join(model_index)

  # find train file
  for index, item in enumerate(train_files):
    if item == model_index:
        found = True
        train_index = index
        break

  # find test file
  for index, item in enumerate(test_files):
    if item == model_index:
        found = True
        test_index = index
        break

  # skip .csv from model_index (previously needed)
  model_index = model_index[0:-4]

  # read files
  train = pd.read_csv(path_train + train_files[train_index])
  test = pd.read_csv(path_test + test_files[test_index])

  # delete column created by R exportation
  train = train.loc[:, train.columns != 'Unnamed: 0']
  test = test.loc[:, test.columns != 'Unnamed: 0']

  # divide into X and Y train, validation and test - 60% / 20% / 20%
  X_train = train.loc[:, train.columns != 'dead']
  y_train = train['dead']

  X_val = X_train.iloc[0:len(test)]
  X_train = X_train.iloc[len(test):]
  y_val = y_train.iloc[0:len(test)]
  y_train = y_train.iloc[len(test):]

  X_test = test.loc[:, test.columns != 'dead']
  y_test = test['dead']

  # calculate class weights using sklearn
  sklearn_weigths = class_weight.compute_class_weight(class_weight='balanced',
                                                      classes=np.unique(y_train),
                                                      y=y_train)
  sklearn_weigths = dict(enumerate(sklearn_weigths))

  # convert data to tensors
  X_train = tf.convert_to_tensor(X_train)
  y_train = tf.convert_to_tensor(y_train)
  X_val = tf.convert_to_tensor(X_val)
  y_val = tf.convert_to_tensor(y_val)
  X_test = tf.convert_to_tensor(X_test)
  y_test = tf.convert_to_tensor(y_test)

  # record time to fit the model
  start = time.time()

  # run the model
  history, preds = fit_test_model_7(X_train=X_train, X_test=X_test,
                                    y_train=y_train, y_test=y_test,
                                    X_val=X_val, y_val=y_val,
                                    weights=sklearn_weigths)

  # record time to fit the model
  end = time.time()
  time_list[model_index] = end - start

  # change predictions format
  preds = preds.tolist()

  # print a message
  print('')
  print(f'Model called', model_index, 'run succesfully.')
  print('')

  # save the output - predictions
  output_path = '/home/aitorvazquez/PhD/Vitality/3_final/data/5_analysis/ann/preds/D', df_size, '_V', var_size, '/', model_index, '.json'
  output_path = ''.join(output_path)
  json.dump(preds, open(output_path, 'w'))

  # save the output - history
  output_path = '/home/aitorvazquez/PhD/Vitality/3_final/data/5_analysis/ann/history/D', df_size, '_V', var_size, '/', model_index, '.npy'
  output_path = ''.join(output_path)
  np.save(output_path, history.history)
  # to load the history, the next line is needed: https://stackoverflow.com/questions/41061457/keras-how-to-save-the-training-history-attribute-of-the-history-object
  # history=np.load('my_history.npy',allow_pickle='TRUE').item()

# record time
end_ann = time.time()
time_list['ANN'] = end_ann - start_ann
output_path = '/home/aitorvazquez/PhD/Vitality/3_final/data/5_analysis/ann/timer/D{}_V{}/ann_time_list.json'.format(df_size, var_size)
json.dump(time_list, open(output_path, 'w'))

# print a message
print('Work finished.')
